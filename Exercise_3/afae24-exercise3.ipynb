{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12394aa1-0fe2-49d1-951a-6defb73ef28c",
   "metadata": {},
   "source": [
    "# Algorithmic Fairness, Accountability, and Ethics, Spring 2024\n",
    "# Exercise 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccc72700",
   "metadata": {},
   "source": [
    "## Task 0 (Setup)\n",
    "\n",
    "We use the same dataset as in week 2. If you missed to install the module, please carry out the installation tasks at <https://github.com/zykls/folktables#basic-installation-instructions>.\n",
    "\n",
    "After successful installation, you should be able to run the following code to generate a prediction task.\n",
    "To make your life easier, we made the `BasicProblem`-magic from the `folktables` package (see exercises of week 2) explicit in this task.\n",
    "This way, you can get access to different encodings of the data. \n",
    "\n",
    "**Note**: Some Windows users could not run the line `acs_data = data_source.get_data(states=[\"CA\"], download=True)`. The dataset is available as a zip file on learnIT under week 2. Unzip it in the notebook's location, and set `download` to `False` in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5921f706-cf73-4c13-a7da-0448ef057d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from folktables.acs import adult_filter\n",
    "from folktables import ACSDataSource\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "data_source = ACSDataSource(survey_year='2018', horizon='1-Year', survey='person', root_dir=\"/mnt/storage/Datasets\")\n",
    "acs_data = data_source.get_data(states=[\"CA\"], download=True)\n",
    "\n",
    "feature_names = ['AGEP', # Age\n",
    "                 \"CIT\", # Citizenship status\n",
    "                 'COW', # Class of worker\n",
    "                 \"ENG\", # Ability to speak English\n",
    "                 'SCHL', # Educational attainment\n",
    "                 'MAR', # Marital status\n",
    "                 \"HINS1\", # Insurance through a current or former employer or union\n",
    "                 \"HINS2\", # Insurance purchased directly from an insurance company\n",
    "                 \"HINS4\", # Medicaid\n",
    "                 \"RAC1P\", # Recoded detailed race code\n",
    "                 'SEX']\n",
    "\n",
    "target_name = \"PINCP\" # Total person's income\n",
    "\n",
    "def data_processing(data, features, target_name:str, threshold: float = 35000):\n",
    "    df = data\n",
    "    ### Adult Filter (STARTS) (from Foltktables)\n",
    "    df = df[~df[\"SEX\"].isnull()]\n",
    "    df = df[~df[\"RAC1P\"].isnull()]\n",
    "    df = df[df['AGEP'] > 16]\n",
    "    df = df[df['PINCP'] > 100]\n",
    "    df = df[df['WKHP'] > 0]\n",
    "    df = df[df['PWGTP'] >= 1]\n",
    "    ### Adult Filter (ENDS)\n",
    "    ### Groups of interest\n",
    "    sex = df[\"SEX\"].values\n",
    "    ### Target\n",
    "    df[\"target\"] = df[target_name] > threshold\n",
    "    target = df[\"target\"].values\n",
    "    df = df[features + [\"target\", target_name]] ##we want to keep df before one_hot encoding to make Bias Analysis\n",
    "    df_processed = df[features].copy()\n",
    "    cols = [ \"HINS1\", \"HINS2\", \"HINS4\", \"CIT\", \"COW\", \"SCHL\", \"MAR\", \"SEX\", \"RAC1P\"]\n",
    "    df_processed = pd.get_dummies(df_processed, prefix=None, prefix_sep='_', dummy_na=False, columns=cols, drop_first=True)\n",
    "    df_processed = pd.get_dummies(df_processed, prefix=None, prefix_sep='_', dummy_na=True, columns=[\"ENG\"], drop_first=True)\n",
    "    return df_processed, df, target, sex\n",
    "\n",
    "data, data_original, target, group = data_processing(acs_data, feature_names, target_name)\n",
    "\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n",
    "    data, target, group, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770eebde",
   "metadata": {},
   "source": [
    "# Task 1 (Logistic regression)\n",
    "\n",
    "1) Train a logistic regression classifier on the training dataset. In our setup, the following parameters worked out well: `LogisticRegression(max_iter=5000, penalty = \"l2\", C= 0.8497534359086438, tol=1e-4, solver = \"saga\")`. Which scaling considerations do you think are necessary?\n",
    "2) Report on the accuracy of the model. (If you are interested: How is the classification accuracy on the original dataset with categorial input?)\n",
    "3) Report on the model weights (sort them by weight). Which weights are most important? Explain the influence of the most important weights. (For example, \"being female instead of male increases/decreases the odds for ... by ...\".)\n",
    "4) Find a negative or a positive instance, and discuss how you can use the weights discussion to create a counterfactual. (E.g., \"By increasing/decreasing feature ... to ..., the person is classified as ...\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7be4571e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7694528914215624\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logit_model = LogisticRegression(max_iter=5000, random_state=42).fit(X_train, y_train)\n",
    "\n",
    "print(logit_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2012fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_name</th>\n",
       "      <th>odds_ratios</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>SCHL_24.0</td>\n",
       "      <td>11.453122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>SCHL_23.0</td>\n",
       "      <td>11.100707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>SCHL_22.0</td>\n",
       "      <td>9.194668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>SCHL_21.0</td>\n",
       "      <td>5.566955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>SCHL_20.0</td>\n",
       "      <td>2.644276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HINS4_2</td>\n",
       "      <td>2.306004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>SCHL_19.0</td>\n",
       "      <td>1.981240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>SCHL_18.0</td>\n",
       "      <td>1.890153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>COW_5.0</td>\n",
       "      <td>1.844063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>SCHL_17.0</td>\n",
       "      <td>1.807154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>SCHL_16.0</td>\n",
       "      <td>1.524488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>COW_7.0</td>\n",
       "      <td>1.309939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>SCHL_12.0</td>\n",
       "      <td>1.245208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>SCHL_3.0</td>\n",
       "      <td>1.214624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>SCHL_15.0</td>\n",
       "      <td>1.199530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>SCHL_10.0</td>\n",
       "      <td>1.173330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SCHL_2.0</td>\n",
       "      <td>1.133598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>ENG_nan</td>\n",
       "      <td>1.092303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>SCHL_11.0</td>\n",
       "      <td>1.084922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>COW_4.0</td>\n",
       "      <td>1.073197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>SCHL_13.0</td>\n",
       "      <td>1.071094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CIT_4</td>\n",
       "      <td>1.069403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>COW_3.0</td>\n",
       "      <td>1.048434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AGEP</td>\n",
       "      <td>1.038104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>MAR_3</td>\n",
       "      <td>1.034665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>SCHL_9.0</td>\n",
       "      <td>1.031276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CIT_3</td>\n",
       "      <td>1.006637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>SCHL_7.0</td>\n",
       "      <td>0.999219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>RAC1P_4</td>\n",
       "      <td>0.993332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>COW_2.0</td>\n",
       "      <td>0.964160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>RAC1P_6</td>\n",
       "      <td>0.963294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>RAC1P_7</td>\n",
       "      <td>0.951879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>RAC1P_8</td>\n",
       "      <td>0.949612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>SCHL_6.0</td>\n",
       "      <td>0.947972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>RAC1P_9</td>\n",
       "      <td>0.899707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>SCHL_14.0</td>\n",
       "      <td>0.890993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HINS2_2</td>\n",
       "      <td>0.890445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CIT_2</td>\n",
       "      <td>0.884704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CIT_5</td>\n",
       "      <td>0.861383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>SCHL_8.0</td>\n",
       "      <td>0.839212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>RAC1P_3</td>\n",
       "      <td>0.837910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>MAR_2</td>\n",
       "      <td>0.820834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>RAC1P_5</td>\n",
       "      <td>0.809832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>MAR_4</td>\n",
       "      <td>0.806939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>RAC1P_2</td>\n",
       "      <td>0.804031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>ENG_2.0</td>\n",
       "      <td>0.747756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>SCHL_5.0</td>\n",
       "      <td>0.709087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>SCHL_4.0</td>\n",
       "      <td>0.599278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>COW_6.0</td>\n",
       "      <td>0.538009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>ENG_3.0</td>\n",
       "      <td>0.522518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>MAR_5</td>\n",
       "      <td>0.490112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>SEX_2</td>\n",
       "      <td>0.440672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>ENG_4.0</td>\n",
       "      <td>0.378953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HINS1_2</td>\n",
       "      <td>0.361862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>COW_8.0</td>\n",
       "      <td>0.287848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     var_name  odds_ratios\n",
       "37  SCHL_24.0    11.453122\n",
       "36  SCHL_23.0    11.100707\n",
       "35  SCHL_22.0     9.194668\n",
       "34  SCHL_21.0     5.566955\n",
       "33  SCHL_20.0     2.644276\n",
       "3     HINS4_2     2.306004\n",
       "32  SCHL_19.0     1.981240\n",
       "31  SCHL_18.0     1.890153\n",
       "11    COW_5.0     1.844063\n",
       "30  SCHL_17.0     1.807154\n",
       "29  SCHL_16.0     1.524488\n",
       "13    COW_7.0     1.309939\n",
       "25  SCHL_12.0     1.245208\n",
       "16   SCHL_3.0     1.214624\n",
       "28  SCHL_15.0     1.199530\n",
       "23  SCHL_10.0     1.173330\n",
       "15   SCHL_2.0     1.133598\n",
       "54    ENG_nan     1.092303\n",
       "24  SCHL_11.0     1.084922\n",
       "10    COW_4.0     1.073197\n",
       "26  SCHL_13.0     1.071094\n",
       "6       CIT_4     1.069403\n",
       "9     COW_3.0     1.048434\n",
       "0        AGEP     1.038104\n",
       "39      MAR_3     1.034665\n",
       "22   SCHL_9.0     1.031276\n",
       "5       CIT_3     1.006637\n",
       "20   SCHL_7.0     0.999219\n",
       "45    RAC1P_4     0.993332\n",
       "8     COW_2.0     0.964160\n",
       "47    RAC1P_6     0.963294\n",
       "48    RAC1P_7     0.951879\n",
       "49    RAC1P_8     0.949612\n",
       "19   SCHL_6.0     0.947972\n",
       "50    RAC1P_9     0.899707\n",
       "27  SCHL_14.0     0.890993\n",
       "2     HINS2_2     0.890445\n",
       "4       CIT_2     0.884704\n",
       "7       CIT_5     0.861383\n",
       "21   SCHL_8.0     0.839212\n",
       "44    RAC1P_3     0.837910\n",
       "38      MAR_2     0.820834\n",
       "46    RAC1P_5     0.809832\n",
       "40      MAR_4     0.806939\n",
       "43    RAC1P_2     0.804031\n",
       "51    ENG_2.0     0.747756\n",
       "18   SCHL_5.0     0.709087\n",
       "17   SCHL_4.0     0.599278\n",
       "12    COW_6.0     0.538009\n",
       "52    ENG_3.0     0.522518\n",
       "41      MAR_5     0.490112\n",
       "42      SEX_2     0.440672\n",
       "53    ENG_4.0     0.378953\n",
       "1     HINS1_2     0.361862\n",
       "14    COW_8.0     0.287848"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = pd.DataFrame({\"var_name\":X_train.columns,\n",
    "                        \"odds_ratios\": np.exp(logit_model.coef_).reshape(-1)}).sort_values(by=\"odds_ratios\", ascending=False)\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d400bf53",
   "metadata": {},
   "source": [
    "# Task 2 (Decision tree)\n",
    "\n",
    "1. Train a decision tree classifier on the training dataset. (You can work on the original dataset or on the one-hot encoded one.) The following parameter choices worked well in our setup: `(DecisionTreeClassifier(min_samples_split = 0.01, min_samples_leaf= 0.01, max_features=\"auto\", max_depth = 15, criterion = \"gini\", random_state = 0))` Report on its accuracy. Visualize the tree using `plot_tree` from `sklearn`. Which parameters can you change the adapt the size of the tree? Try to find parameters that make the tree easier to understand.\n",
    "2. For two training examples, explain their classification given the decision tree.\n",
    "3. Compute feature importance as shown in the lecture. Which features are most important?\n",
    "4. Provide a counterfactual, as in Task 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7864fb8",
   "metadata": {},
   "source": [
    "# Task 3 (Comparison)\n",
    "\n",
    "Now you have both an interpretable logistic regression model and an interpretable decision tree model. Reflect on the explanations you can obtain from these two models: Do explanations from one model translate to the other? Do the counterfactuals from one work in the other model?  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
