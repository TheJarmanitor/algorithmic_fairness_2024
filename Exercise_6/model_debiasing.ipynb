{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folktables.acs import adult_filter\n",
    "from folktables import ACSDataSource, BasicProblem, generate_categories\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score, matthews_corrcoef\n",
    "from sklearn.model_selection import KFold\n",
    "import scipy.optimize as opt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and Preprocess the data\n",
    "We are going to work with the [Folktables](https://github.com/socialfoundations/folktables#quick-start-examples) dataset (*you have already worked with it*). I have chosen some variables for you, but you can add more (*if you like to*) - here is the [full list](https://www2.census.gov/programs-surveys/acs/tech_docs/pums/data_dict/PUMS_Data_Dictionary_2021.pdf) of variables (some of them do not exist in `ACSDataSource`). \n",
    "\n",
    "Today we are going to debias a regression model using the `SEX` variable. Your model should predict the *Total person's income*  (I've digitized  it in  `target_transform=lambda x: x > 25000`, you can choose another threshold).\n",
    "\n",
    "\n",
    "* If you code is slow - you can subsample data (aka reduce the number of the samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data for 2018 1-Year person survey for CA...\n"
     ]
    }
   ],
   "source": [
    "data_source = ACSDataSource(survey_year='2018', horizon='1-Year', survey='person', root_dir='/mnt/storage/folktables')\n",
    "acs_data = data_source.get_data(states=[\"CA\"], download=True)\n",
    "\n",
    "ACSIncomeNew = BasicProblem(\n",
    "    features=[\n",
    "        'AGEP', # include AGE\n",
    "        'COW', # include class of worker\n",
    "        'SCHL', # include school education\n",
    "        'WKHP', # include reported working hours\n",
    "        'SEX', # include sex\n",
    "        # some random, possibly noisy\n",
    "        'PWGTP', # person weight\n",
    "        'JWMNP', # travel time to work\n",
    "    ],\n",
    "    target='PINCP',\n",
    "    target_transform=lambda x: x > 25000,    \n",
    "    group='SEX',\n",
    "    preprocess=adult_filter,\n",
    "    postprocess=lambda x: np.nan_to_num(x, -1),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a small snippet to get the names of the categorical variables - I convert categoricals into one-hot encoded (*you don't have to, depending on what assumptions you use about the data*). **Don't forget to normalise the continious features (if you plan to use Cross-Validation features should be normalized per fold, aka not in the global table).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10835/3300939264.py:5: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  features = features.fillna(-1) # Fill nulls with -1 which becomes necessary for the optimization\n",
      "/tmp/ipykernel_10835/3300939264.py:8: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0        -0.855575\n",
      "1        -1.460226\n",
      "2         1.495845\n",
      "3        -0.654025\n",
      "4        -1.661776\n",
      "            ...   \n",
      "195660   -0.318108\n",
      "195661   -0.250924\n",
      "195662    1.227111\n",
      "195663    1.764579\n",
      "195664   -0.183741\n",
      "Name: AGEP, Length: 195665, dtype: float64' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  features.iloc[:,:4] = ((features.iloc[:,:4] - features.iloc[:,:4].mean()) / features.iloc[:,:4].std()).astype(\"float64\")\n",
      "/tmp/ipykernel_10835/3300939264.py:8: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0        -0.977862\n",
      "1        -0.711460\n",
      "2        -0.964542\n",
      "3        -0.698140\n",
      "4         0.007825\n",
      "            ...   \n",
      "195660   -0.511659\n",
      "195661   -0.511659\n",
      "195662    0.207627\n",
      "195663   -0.365138\n",
      "195664    0.886952\n",
      "Name: PWGTP, Length: 195665, dtype: float64' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  features.iloc[:,:4] = ((features.iloc[:,:4] - features.iloc[:,:4].mean()) / features.iloc[:,:4].std()).astype(\"float64\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGEP</th>\n",
       "      <th>WKHP</th>\n",
       "      <th>PWGTP</th>\n",
       "      <th>JWMNP</th>\n",
       "      <th>COW_Employee of a private for-profit company or business, or of an individual, for wages, salary, or commissions</th>\n",
       "      <th>COW_Employee of a private not-for-profit, tax-exempt, or charitable organization</th>\n",
       "      <th>COW_Federal government employee</th>\n",
       "      <th>COW_Local government employee (city, county, etc.)</th>\n",
       "      <th>COW_Self-employed in own incorporated business, professional practice or farm</th>\n",
       "      <th>COW_Self-employed in own not incorporated business, professional practice, or farm</th>\n",
       "      <th>...</th>\n",
       "      <th>SCHL_Grade 9</th>\n",
       "      <th>SCHL_Kindergarten</th>\n",
       "      <th>SCHL_Master's degree</th>\n",
       "      <th>SCHL_No schooling completed</th>\n",
       "      <th>SCHL_Nursery school, preschool</th>\n",
       "      <th>SCHL_Professional degree beyond a bachelor's degree</th>\n",
       "      <th>SCHL_Regular high school diploma</th>\n",
       "      <th>SCHL_Some college, but less than 1 year</th>\n",
       "      <th>SEX_Female</th>\n",
       "      <th>SEX_Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.855575</td>\n",
       "      <td>0.163863</td>\n",
       "      <td>-0.977862</td>\n",
       "      <td>-1.028986</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.460226</td>\n",
       "      <td>-1.372348</td>\n",
       "      <td>-0.711460</td>\n",
       "      <td>-0.595975</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.495845</td>\n",
       "      <td>-2.294075</td>\n",
       "      <td>-0.964542</td>\n",
       "      <td>-0.005505</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.654025</td>\n",
       "      <td>0.163863</td>\n",
       "      <td>-0.698140</td>\n",
       "      <td>-1.028986</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.661776</td>\n",
       "      <td>-1.525969</td>\n",
       "      <td>0.007825</td>\n",
       "      <td>-1.028986</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       AGEP      WKHP     PWGTP     JWMNP  \\\n",
       "0 -0.855575  0.163863 -0.977862 -1.028986   \n",
       "1 -1.460226 -1.372348 -0.711460 -0.595975   \n",
       "2  1.495845 -2.294075 -0.964542 -0.005505   \n",
       "3 -0.654025  0.163863 -0.698140 -1.028986   \n",
       "4 -1.661776 -1.525969  0.007825 -1.028986   \n",
       "\n",
       "   COW_Employee of a private for-profit company or business, or of an individual, for wages, salary, or commissions  \\\n",
       "0                                              False                                                                  \n",
       "1                                              False                                                                  \n",
       "2                                              False                                                                  \n",
       "3                                               True                                                                  \n",
       "4                                              False                                                                  \n",
       "\n",
       "   COW_Employee of a private not-for-profit, tax-exempt, or charitable organization  \\\n",
       "0                                              False                                  \n",
       "1                                              False                                  \n",
       "2                                               True                                  \n",
       "3                                              False                                  \n",
       "4                                               True                                  \n",
       "\n",
       "   COW_Federal government employee  \\\n",
       "0                            False   \n",
       "1                            False   \n",
       "2                            False   \n",
       "3                            False   \n",
       "4                            False   \n",
       "\n",
       "   COW_Local government employee (city, county, etc.)  \\\n",
       "0                                              False    \n",
       "1                                              False    \n",
       "2                                              False    \n",
       "3                                              False    \n",
       "4                                              False    \n",
       "\n",
       "   COW_Self-employed in own incorporated business, professional practice or farm  \\\n",
       "0                                              False                               \n",
       "1                                              False                               \n",
       "2                                              False                               \n",
       "3                                              False                               \n",
       "4                                              False                               \n",
       "\n",
       "   COW_Self-employed in own not incorporated business, professional practice, or farm  \\\n",
       "0                                               True                                    \n",
       "1                                              False                                    \n",
       "2                                              False                                    \n",
       "3                                              False                                    \n",
       "4                                              False                                    \n",
       "\n",
       "   ...  SCHL_Grade 9  SCHL_Kindergarten  SCHL_Master's degree  \\\n",
       "0  ...         False              False                 False   \n",
       "1  ...         False              False                 False   \n",
       "2  ...         False              False                  True   \n",
       "3  ...         False              False                 False   \n",
       "4  ...         False              False                 False   \n",
       "\n",
       "   SCHL_No schooling completed  SCHL_Nursery school, preschool  \\\n",
       "0                        False                           False   \n",
       "1                        False                           False   \n",
       "2                        False                           False   \n",
       "3                        False                           False   \n",
       "4                        False                           False   \n",
       "\n",
       "   SCHL_Professional degree beyond a bachelor's degree  \\\n",
       "0                                              False     \n",
       "1                                              False     \n",
       "2                                              False     \n",
       "3                                              False     \n",
       "4                                              False     \n",
       "\n",
       "   SCHL_Regular high school diploma  SCHL_Some college, but less than 1 year  \\\n",
       "0                             False                                    False   \n",
       "1                              True                                    False   \n",
       "2                             False                                    False   \n",
       "3                             False                                    False   \n",
       "4                             False                                    False   \n",
       "\n",
       "   SEX_Female  SEX_Male  \n",
       "0       False      True  \n",
       "1       False      True  \n",
       "2       False      True  \n",
       "3       False      True  \n",
       "4        True     False  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "definition_df = data_source.get_definitions(download=True)\n",
    "categories = generate_categories(features=ACSIncomeNew.features, definition_df=definition_df)\n",
    "# Here I convert categoricals into one-hot encoded (you don't have to, depending on what assumptions you use about the data)\n",
    "features, labels, groups = ACSIncomeNew.df_to_pandas(acs_data, categories=categories, dummies=True)\n",
    "features = features.fillna(-1) # Fill nulls with -1 which becomes necessary for the optimization\n",
    "\n",
    "########### Normalize continious features\n",
    "features.iloc[:,:4] = ((features.iloc[:,:4] - features.iloc[:,:4].mean()) / features.iloc[:,:4].std())\n",
    "## YOUR CODE (if relevant)\n",
    "###########\n",
    "features.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliting data into train-test. **Again, if you plan to use Cross-Validation then you should normalise features only inside of a fold**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n",
    "    features.values, labels.values.reshape(-1), groups.values.reshape(-1), test_size=0.3, random_state=0, shuffle=True)\n",
    "\n",
    "N = 1000 ### I am subsampling because it is slow on my machine\n",
    "X_train = X_train[:N]\n",
    "y_train = y_train[:N]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  Regression model (without Fairness constraints)\n",
    "Let's first train a simple **Logistic Regression**. \n",
    "1. Use L2 penalty to train the model (you should find the optimal value for the regularizer)\n",
    "2. Calculate the total performance metric\n",
    "3. Calculate and compare the performance metric for each `SEX` group (use your favourite metric introduced during the course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.797\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"score: {round(clf.score(X_test, y_test),4)}\")\n",
    "\n",
    "cm = confusion_matrix(y_test, clf.predict(X_test))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Constrained Regression Model \n",
    "Now let's try to include the [Fairness Constraint](https://arxiv.org/abs/1706.02409)! You'll have to implement couple of things from scratch (as it is tricky to add a custom constraint function in `sklearn`.  To optimise the cost function let's use `scipy.optimize.fmin_tnc`. To calculate gradient you can use `fprime` attribute):\n",
    "1. Logistic Regression\n",
    "2. L2 penalisation\n",
    "3. **Individual** Fairness Constrained\n",
    "\n",
    "When you are finished with the implementation - you should evaluate performance on multiple choices of fairness weight, $\\lambda$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detailed breakdown\n",
    "The INDIVIDUAL constraint constraint looks like this:\n",
    "\n",
    "$$ \n",
    "f(\\beta,S) = \\left( \\frac{1}{n_1 n_2} \\sum_{(x_i,y_i)\\in S_1, (x_j,y_j)\\in S_2} d(y_i,y_j) (\\beta^T  \\textbf{x}_i - \\beta^T \\textbf{x}_j)^2  \\right) \n",
    "$$\n",
    "\n",
    "\n",
    "For the constrained optimization we have to solve a problem on the form:\n",
    "\n",
    "$$ \\min_\\beta \\left( \\ell (\\beta,S) + \\lambda f(\\beta,S)  +\\gamma \\Vert \\beta \\Vert_2 \\right) $$ \n",
    "\n",
    "where $\\ell$ is some loss function, $f$ is the constraint function, and the $\\gamma \\Vert \\beta \\Vert_2 $ is L2 regularization (we use it to avoid overfitting).\n",
    "(Basically we are minimizing the Lagrangian $\\mathscr{L} = \\ell (\\beta,S) + \\lambda f(\\beta,S)  +\\gamma \\Vert \\textbf{x} \\Vert_2$ with respect to $\\beta$ - in ML literature $\\mathscr{L}$ is often denoted as J)\n",
    "\n",
    "Because we are doing classification we are going to use logistic regression. The log loss function is:\n",
    "$$\n",
    "\\ell = \\frac{1}{m}\\sum_i^m\\left[ -y_i \\log(g(x_i)) - (1-y_i)\\log(1-g(x_i)) \\right], \\text{where } g(x_i) = \\frac{1}{1+\\exp(-\\beta_i x_i)}\n",
    "$$\n",
    "\n",
    "For the distance function we follow the approach from Berk et al. (2017) and set:\n",
    "$$d(y_i,y_j) = \\begin{cases}\n",
    "            1, &         \\text{if } y_i=y_j,\\\\\n",
    "            0, &         \\text{if } y_i\\neq y_j.\n",
    "    \\end{cases}$$\n",
    "    \n",
    "To minimize the total loss function we also need to estimate the gradient of $\\mathscr{L}$ with respect to $\\beta$. Here to update the $\\beta$ values we are just going the gradient's without the fairness constraing - this will make our lives considerably easier. The j'th element of the gradiend is defined as follows:\n",
    "$$\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial \\beta_j} \\approx \\frac{1}{m}\\left( \\sum_i  (g(x_i) - y_i) x[j] \\right)+ 2\\gamma \\beta_j\n",
    "$$\n",
    "\n",
    "##### A little clarification and tips:\n",
    "1. In order to simplify the exercise - we cut some corners. *Ideally* we should calculate the gradient in respect to the *individual fairness*. The gradient takes into the account only logistic and l2 loss (aka, parameters are updated based on those). At the same time, our *cost* has a *individual fairness* included. When the update of the parameters stops decreasing the cost, the `fmin_tnc` is going to stop optimisation. So our implementation is not entirely correct.\n",
    "2. In case you want to have a more correct implementation, you can do `opt.fmin_tnc(func=compute_cost, x0=betas, fprime = None, approx_grad= True, ...)`. It is quite long, but you can still do it\n",
    "3. I also suggest setting `ftol=1e-5`. \n",
    "4. Don not apply l2-regularization on the intercept (when you calculate the gradient).\n",
    "5. You should include $x_0 = 1$ in your data, for each observation (when it comes to the manual implementation of logistic regression) to include bias (i.e. weight $\\beta_0$).\n",
    "6. To keep the exercise simpler, let's fix $\\gamma = 1e-5$.\n",
    "7. Try $lambda$ is a range from around $1$ to $1e5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    This is logistic regression\n",
    "    f = 1/(1+exp(-beta^T * x))\n",
    "    This function assumes as input that you have already multiplied beta and X together\n",
    "    \"\"\"\n",
    "    NotImplemented\n",
    "\n",
    "def logistic_loss(y_true, y_pred, eps = 1e-10):\n",
    "    \"\"\"\n",
    "    Loss for the logistic regression, y_preds are probabilities\n",
    "    eps: epsilon for stability\n",
    "    \"\"\"\n",
    "    NotImplemented\n",
    "\n",
    "def l2_loss(beta):\n",
    "    \"\"\"\n",
    "    L2-Regularisation\n",
    "    \"\"\"\n",
    "    NotImplemented\n",
    "\n",
    "def fair_loss(y, y_pred, groups):\n",
    "    \"\"\"\n",
    "    Group fairness Loss\n",
    "    \"\"\"\n",
    "    n = y.shape[0]\n",
    "    n1 = np.sum(groups == 1)\n",
    "    n2 = np.sum(groups == 2)\n",
    "    cost = 0\n",
    "    NotImplemented\n",
    "    return (cost/(n1*n2))\n",
    "\n",
    "def compute_gradient(beta,X,y, groups, _lambda,_gamma):\n",
    "    \"\"\"Calculate the gradient - used for finding the best beta values. \n",
    "       You do not need to use groups and lambda (fmin_tnc expects same input as in func, that's why they are included here)\"\"\"\n",
    "    grad = np.zeros(beta.shape)\n",
    "    NotImplemented\n",
    "\n",
    "    for i in range(len(grad)):\n",
    "        if i == 0: # we do not want to regularize the intercept\n",
    "            grad[i] =  ...\n",
    "        else:\n",
    "            grad[i] = ...\n",
    "        \n",
    "    return grad\n",
    "\n",
    "def compute_cost(beta ,X,y, groups, _lambda, _gamma):\n",
    "    \"\"\"Computes cost function with constraints\"\"\"\n",
    "    NotImplemented\n",
    "    probs = sigmoid(X.dot(beta))\n",
    "    loss = logistic_loss(y, probs) + _lambda * fair_loss(y,X.dot(beta), groups) + _gamma * l2_loss(beta[1:])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cost with random beta-values and parameters\n",
    "compute_cost(\n",
    "    beta = np.random.rand(X_train.shape[1]),\n",
    "    X = X_train, \n",
    "    y = y_train,\n",
    "    groups = group_train, \n",
    "    _gamma = 1, \n",
    "    _lambda = 1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run optimization with single `lambda` and `gamma` values\n",
    "### Set seed and define params\n",
    "np.random.seed(0)\n",
    "beta = np.random.rand(X_train.shape[1])\n",
    "lambda_ = # `1000` worked for robustly scaled continuous features\n",
    "gamma_ = # `1e-5` worked for robustly scaled continuous features\n",
    "\n",
    "### Run optimization\n",
    "result, _, _ = opt.fmin_tnc(\n",
    "    func=compute_cost,\n",
    "    x0=beta,\n",
    "    fprime=compute_gradient,\n",
    "    maxfun = 500,\n",
    "    args = (\n",
    "        X_train, \n",
    "        y_train,\n",
    "        group_train,\n",
    "        lambda_, \n",
    "        gamma_\n",
    "    ),\n",
    "    xtol=1e-7,\n",
    "    ftol=1e-5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
